{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RAIL - Fundamentals\n",
    "\n",
    "RAIL is a LSST-DESC software created to process different algorithms used to calculate photometric redshift. Its main goal is to minimize impact that different infrastructures can cause on different algorithms, for that it unifyes in a modular code supporting different inputs that different algorithms needs and padronizing the output so that it can be a more fair comparison between their results.\n",
    "\n",
    "Rail uses 4 principal libraries in its core: <br>\n",
    "_tables_io_: for data manipulation as hdf5 files, fits, etc. <br>\n",
    "_qp_: used to paremitrize data PDFs for metrics calculation. <br>\n",
    "_ceci_: construct pipelines, produces a .yaml within the steps and configurations as threads. <br>\n",
    "_pzflow_: creates a flow for data creation. <br>\n",
    "\n",
    "#### Core.\n",
    "Where the main functions are going to manage the data and files that the program creates. It works based in the behavioral chain of resposability pattern (https://refactoring.guru/pt-br/design-patterns/chain-of-responsibility), where you create a flux in the code, where there is a request related/processed by a class handler that decides to pass it foward or not according to what is defined. So for that, what bpz does is create a class request (eg: Inform_BPZ_lite) that has all the inputs/configurations and is handled by its class handler (BPZ_lite).\n",
    "\n",
    "#### Creation.\n",
    "Contain all the support for data creation, as degradors, data flow creation, Column remapping, etc. It creates .hdf5 files with the data that is being manipulated.\n",
    "\n",
    "#### Estimation.\n",
    "This is where the codes are defined and executed.  <br>\n",
    "inform: this is where the PRIORS for template fitting are informed and the machine learning codes are trained. <br>\n",
    "estimate: where the algorith is executed though the .evaluate() function.\n",
    "The code is wrapped as a RAIL stage so that it can be run in a controlled way. Estimation code can be stored in a yaml file to be run as a ceci module.\n",
    "\n",
    "\n",
    "#### Evaluation.\n",
    "This step contais the metrics for performance of the estimated codes.\n",
    "<br>\n",
    "------\n",
    "For installation instructions check the official documentation: https://lsstdescrail.readthedocs.io/en/latest/source/installation.html <br>\n",
    "\n",
    "Its important to point out that as Rail is still being developed it may be necessary to do a update (onde in a while) to you rail package once its installed. <br> \n",
    "First you must update the cloned rail repository: _git pull origin_ <br>\n",
    "Then, run: pip install packageName --upgrade\n",
    "\n",
    "For Rail versions check: https://github.com/LSSTDESC/RAIL/releases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports, setup and some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import qp\n",
    "import tables_io\n",
    "\n",
    "import rail\n",
    "\n",
    "##from rail.core.data import TableHandle, PqHandle, ModelHandle\n",
    "##from rail.core.stage import RailStage\n",
    "##from rail.core.utilStages import ColumnMapper, TableConverter\n",
    "\n",
    "##from rail.estimation.algos.bpz_lite import Inform_BPZ_lite, BPZ_lite\n",
    "##from rail.evaluation.evaluator import Evaluator\n",
    "\n",
    "#for rail versions\n",
    "help(rail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_DIR = os.getcwd()\n",
    "RAIL_DIR = os.path.join(os.path.dirname(rail.__file__), '..')\n",
    "CURR_DIR, RAIL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reading some sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = [\"coadd_objects_id\",\"ra\",\"dec\",\"mag_g\",\"magerr_g\",\"mag_i\",\"magerr_i\",\"mag_r\",\"magerr_r\",\"mag_u\",\"magerr_u\",\"mag_y\",\"magerr_y\",\"mag_z\",\"magerr_z\",\"z_true\"]\n",
    "\n",
    "file_path = '../../../../DATA/dp0_train_random.csv'\n",
    "full_data = pd.read_csv(file_path, usecols=data_columns)\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spliting into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(full_data)//2\n",
    "\n",
    "train_sample = full_data.sample(n=size,ignore_index=True)\n",
    "test_sample = full_data.drop(train_sample.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##  RAIL \n",
    "\n",
    "Rail has a lot of classes and it uses Object Oriented Programming - POO, therefore things can get complicated very fast, but for now we are going to focus on understanging a little bit of the three bases ones: **RailStage, DataStore and DataHandler**\n",
    "\n",
    "**Image:** This diagram represents some classes and its hierarchy.\n",
    "\n",
    "![title](RAILclasses.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data store class is the class that is going to store all the data that is being processed associated with a key value. For example for a file containing the sample that we are going to use to test an algorithm named 'test_sampe.hdf5' we add this to the data store naming the key 'test_sample' and a what class (DaataHandler) is it going to use to read it, in this case TableHandler -> HandlerHdf5. \n",
    "<br>\n",
    "\n",
    "Another important thing to know is that the DataStore class acts as a [singleton class](https://refactoring.guru/design-patterns/singleton) wich basically is a class that has only one instance in the aplication. That is important due to the fact that rail keeps all the data and handlers as it runs so that the previous stage can access and read it. Based on that when if we try to create another instace, what its going to do is serve as a DataStore factory, but not the DataStore class itself.  \n",
    "<br>\n",
    "\n",
    "We can see access the data storage trough the attribute data_store. By default it does not allow to overwrite the data tha its being stored so if we want to change the value of a key we have to manually set the property allow_overwrite to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True\n",
    "DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us undestand better we are constantly going to monitor how DataStore stores the data and how the memory goes with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.getsizeof(DS), 'bytes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually add data to the data store with the add_data or pass a file and store it in the DS with the read_file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS.add_data(key=\"input\", data=train_sample, handle_class=PqHandle)\n",
    "## DS.read_file(key=\"name\", path=file_path, handle_class=Handler)\n",
    "DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we can access the data, what it is going to do is use the handler that we passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(DS), 'bytes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS.read(\"input\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory x Files\n",
    "\n",
    "As we can see, as soon that we added the data to the DS the memory increased in 200 bytes. In Rail we can store data as a tableLike object as pandas dataframe, orderDic, etc. but we can also work with the flow in memory. For that there are a bunch of steps/configs that are going differ from bringing or not the data to memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all the stages herd from RailStages, [Delegate Pattern](https://en.wikipedia.org/wiki/Delegation_pattern), that can be seen in figure of the classe maped above, and RailStages can be seen as a \n",
    "[CeciStage](https://github.com/LSSTDESC/ceci/blob/d1d5686aefab18bc53e3d4d8a05af42d19e28a91/ceci/stage.py#L24]), when we declare a stage we use the method _make_stage(**args)_, what id does is to return the object itself as a stage configured with the given parameters. \n",
    "\n",
    "To undestand how the returned object works we can use an explanation present in the c# language for [delegate](https://docs.microsoft.com/pt-br/dotnet/csharp/programming-guide/delegates/using-delegates). Basically we can think of delegates as a method that points to an abstract class and a method of that class that is going to execute. Therefore a class that behaves as a method and can be executed. In python this method can be declared as `__call__` and the retuned class can be executed as class(), then this is going to execute the defined methos class. For RailStages it is going to run the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##help(ColumnMapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_remmap = {\n",
    "\"coadd_objects_id\": \"id\",\n",
    "\"ra\": \"coord_ra\",\n",
    "\"dec\": \"coord_dec\",\n",
    "\"mag_g\": \"mag_g_lsst\",\n",
    "\"magerr_g\": \"mag_err_g_lsst\",\n",
    "\"mag_i\": \"mag_r_lsst\",\n",
    "\"magerr_i\": \"mag_err_r_lsst\",\n",
    "\"mag_r\": \"mag_i_lsst\",\n",
    "\"magerr_r\": \"mag_err_i_lsst\",\n",
    "\"mag_u\": \"mag_u_lsst\",\n",
    "\"magerr_u\": \"mag_err_u_lsst\",\n",
    "\"mag_y\": \"mag_y_lsst\",\n",
    "\"magerr_y\": \"mag_err_y_lsst\",\n",
    "\"mag_z\": \"mag_z_lsst\",\n",
    "\"magerr_z\": \"mag_err_z_lsst\",\n",
    "\"z_true\": \"redshift\"\n",
    "}\n",
    "\n",
    "col_remapper_train = ColumnMapper.make_stage(name='col_remapper_train', columns=columns_remmap)\n",
    "print(f\"Returned class -> {type(col_remapper_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(DS), 'bytes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the configurations of the returned class with `returned_obj.config.to_dict()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##col_remapper_train.config.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically we can call execute it in two ways.\n",
    "1. When the data is added manually to the DS with `col_remapper_train.run()`\n",
    "2. Passing the data trough parameter and invoking the method as `col_remapper_train(dataAsTableLike)`\n",
    "\n",
    "in this case we are going to call the method run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_remapper_train.run()\n",
    "print(f\"\\nRodando em paralelo -> {col_remapper_train.is_parallel()}\")\n",
    "DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it is storing the outputs in the DS before the stage. Lets check the outupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_remapper_train.get_data(\"output\").head() \n",
    "## or trough DS as \n",
    "##DS.read(\"output_col_remapper_train\")\n",
    "##DS[\"output_col_remapper_train\"].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(DS), 'bytes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "**OBSERVATION**\n",
    "\n",
    "Passing the input in make_stage does not work\n",
    "`ColumnMapper.make_stage(name='col_remapper_train_2', columns=columns_remmap, input='test')`\n",
    "\n",
    "While a did the test of putting the input as name of DS PqHandler does not work, what it does when we call run what it does primary is to call get_data <br>\n",
    "`data = self.get_data('input', allow_missing=True)` <br>\n",
    "this search in the DS to a key named input.\n",
    "\n",
    "To change that would be necessary to call the method <br>\n",
    "`self.set_data(self.config.input, data)` <br> before and if not set then serach by the key 'input'\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the The algorithms, basically they all expect an input as TableHandler.<br>\n",
    "`inputs = [('input', <class 'rail.core.data.TableHandle'>)]`<br>\n",
    "as the output of remmapColumns is already a TableHandler we dont need to specify, but if the data is already in the correct form, it may be helpful to use the TableConverter class. \n",
    "\n",
    "Eg:\n",
    "\n",
    "     table_conv_train = TableConverter.make_stage(name='table_conv_train', output_format='numpyDict')\n",
    "     table_conv_train.run()\n",
    "\n",
    "\n",
    "and the output is a ModelHandler<br>\n",
    "`outputs = [('model', <class 'rail.core.data.ModelHandle'>)]`\n",
    "\n",
    "**Image:** basic flux of inputs and outputs. \n",
    "\n",
    "![title](SimpleRailBPZflow.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##help(Inform_BPZ_lite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "DS.add_data(key=\"input\", data=col_remapper_train.get_data(\"output\"), handle_class=PqHandle)\n",
    "DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "bpz_columns_file = os.path.join(CURR_DIR, 'configs/bpz.columns')\n",
    "\n",
    "inform_bpz = Inform_BPZ_lite.make_stage(\n",
    "    name='inform_bpzlite', \n",
    "    ##input=\"inprogress_output_col_remapper_train.pq\", same question as above\n",
    "    model='trained_BPZ_output.pkl', \n",
    "    hdf5_groupname='', \n",
    "    columns_file=bpz_columns_file,\n",
    "    prior_band=\"mag_r_lsst\"\n",
    ")\n",
    "inform_bpz.config.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute the best fit prior parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "inform_bpz.run()\n",
    "## or inform_bpz.inform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## POSTERIOR -> Estimate 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For posteriors\n",
    "\n",
    "     inputs = [('model', <class 'rail.core.data.ModelHandle'>)]\n",
    "     outputs = [('output', <class 'rail.core.data.QPHandle'>)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##help(BPZ_lite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS.add_data(key=\"input\", data=DS[\"model_inform_bpzlite\"].data, handle_class=ModelHandle)\n",
    "DS, DS.read(\"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_bpz = BPZ_lite.make_stage(\n",
    "    name='estimate_bpz', \n",
    "    hdf5_groupname='', \n",
    "    columns_file=bpz_columns_file, \n",
    "    ##input=\"inprogress_output_table_conv_train.hdf5\", \n",
    "    model=inform_bpz.get_handle('model')\n",
    ")\n",
    "estimate_bpz.set_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_bpz.run() ## -> input -> DS\n",
    "estimate(dato) ## 'input' -> dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_orig = test_data.data\n",
    "\n",
    "evaluator = Evaluator.make_stage(name=f'bpz_eval', truth=test_data_orig)\n",
    "result_dict = evaluator.evaluate(bpz_estimated, test_data_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_tables = tables_io.convertObj(result_dict.data, tables_io.types.PD_DATAFRAME)\n",
    "results_tables.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CECI pipeline -> undestand this pipeline yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ceci\n",
    "pipe = ceci.Pipeline.interactive()\n",
    "stages = [\n",
    "    # create the test catalog\n",
    "    flow_creator_test, lsst_error_model_test, col_remapper_test, table_conv_test,\n",
    "    # inform the estimators\n",
    "    inform_bpz,\n",
    "    # estimate posteriors\n",
    "    estimate_bpz,\n",
    "    # evaluator\n",
    "    evaluator\n",
    "]\n",
    "for stage in stages:\n",
    "    pipe.add_stage(stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.initialize(dict(input=catalog_file), dict(output_dir='.', log_dir='.', resume=False), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.save('tmp_goldenspike.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = ceci.Pipeline.read('tmp_goldenspike.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
