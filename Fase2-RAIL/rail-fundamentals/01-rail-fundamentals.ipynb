{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img align=\"left\" src = https://linea.org.br/wp-content/themes/LIneA/imagens/logo-header.jpg width=100 style=\"padding: 20px\"> \n",
    "\n",
    "<img align=\"left\" src = https://project.lsst.org/sites/default/files/Rubin-O-Logo_0.png width=160 style=\"padding: 20px\">  \n",
    "\n",
    "# RAIL - Fundamentals\n",
    "\n",
    "**Contact author**: Heloisa da Silva Mengisztki ([heloisasmengisztki@gmail.com](mailto:heloisasmengisztki@gmail.com)) \n",
    "\n",
    "**Last verified run**: 2023-02-01 <br><br>\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "**DISCLAIMER**: This notebook is not part of RAIL official documentation. It is an excercise to explore and learn about RAIL's fundamentals, as part of the education program for undergraduate students (\"Iniciação Científica\") offered by LIneA.  \n",
    "The official documentation is available on [RAIL's Read The Docs page](https://lsstdescrail.readthedocs.io/en/stable/). \n",
    "*** \n",
    "\n",
    "\n",
    "RAIL is a LSST-DESC software created to process different algorithms used to calculate photometric redshift. Its main goal is to minimize impact that different infrastructures can cause on different algorithms. For that, it unifyes in a modular code supporting different inputs that different algorithms needs and padronizing the output so that it can be a more fair comparison between their results.\n",
    "\n",
    "Rail uses 4 principal libraries in its core: <br>\n",
    "* _tables_io_: for data manipulation as hdf5 files, fits, etc. <br>\n",
    "* _qp_: used to paremitrize data PDFs for metrics calculation. <br>\n",
    "* _ceci_: construct pipelines, produces a .yaml within the steps and configurations as threads. <br>\n",
    "* _pzflow_: creates a flow for data creation. <br>\n",
    "\n",
    "RAIL's code base is organized in four subpackges: \n",
    "\n",
    "#### Core.\n",
    "Where the main functions are going to manage the data and files that the program creates. It works almost like the behavioral chain of [resposability pattern](https://refactoring.guru/pt-br/design-patterns/chain-of-responsibility), where you create a flux in the code, where there is a request related/processed by a class handler that decides to pass it foward or not according to what is defined. For instance, to run the BPZ algorithm, one creates a class request (eg: Inform_BPZ_lite) that has all the inputs/configurations and is handled by its class handler (BPZ_lite).\n",
    "\n",
    "#### Creation.\n",
    "Contain all the support for data creation, as degradors, data flow creation, Column remapping, etc. It creates .hdf5 files with the data that is being manipulated.\n",
    "\n",
    "#### Estimation.\n",
    "This is where the codes are defined and executed. The Estimation is separated into two steps:  <br>\n",
    "\n",
    "* **inform:** this is the first step of a photo-z code excecution, where the PRIORS for template fitting are informed and the machine learning codes are trained. <br> \n",
    "\n",
    "* **estimate:** where the algorith is executed though the .evaluate() function. The code is wrapped as a RAIL stage so that it can be run in a controlled way. Estimation code can be stored in a yaml file to be run as a ceci module.\n",
    "\n",
    "\n",
    "#### Evaluation.\n",
    "This step contais the metrics for performance of the estimated codes. It depends on the availability of a truth table for comparison. In case of data created by RAIL Creation subpackage, the truth redshifts and truth posteriors are available by definition.  \n",
    "\n",
    "<br>\n",
    "------\n",
    "For installation instructions check the official [documentation](https://lsstdescrail.readthedocs.io/en/latest/source/installation.html). <br>\n",
    "\n",
    "Its important to point out that as Rail is still being developed it may be necessary to update (once in a while) the packages installed.<br> \n",
    "Commands to update: <br>\n",
    "`pip install pz-rail-bpz --upgrade` <br>\n",
    "`pip install pz-rail --upgrade`\n",
    "\n",
    "For Rail versions check [documentation](https://github.com/LSSTDESC/RAIL/releases)\n",
    "\n",
    "[Adding the kernel to jupyter](https://lsstdescrail.readthedocs.io/en/latest/source/installation.html#adding-your-kernel-to-jupyter): <br>\n",
    " `ipykernel with conda install ipykernel` <br>\n",
    " `python -m ipykernel install –user –name [nametocallnewkernel] `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports, setup and some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import qp\n",
    "import tables_io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import rail\n",
    "from rail.core.utils import RAILDIR\n",
    "from rail.core.data import TableHandle, PqHandle, ModelHandle, QPHandle\n",
    "from rail.core.stage import RailStage\n",
    "from rail.core.utilStages import ColumnMapper, TableConverter\n",
    "\n",
    "from rail.estimation.algos.bpz_lite import Inform_BPZ_lite, BPZ_lite\n",
    "from rail.evaluation.evaluator import Evaluator\n",
    "\n",
    "from rail.estimation.algos.knnpz import Inform_KNearNeighPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_DIR = os.getcwd()\n",
    "CURR_DIR, RAILDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reading some sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = [\"coadd_objects_id\",\"ra\",\"dec\",\"mag_g\",\"magerr_g\",\"mag_i\",\"magerr_i\",\"mag_r\",\"magerr_r\",\"mag_u\",\"magerr_u\",\"mag_y\",\"magerr_y\",\"mag_z\",\"magerr_z\",\"z_true\"]\n",
    "\n",
    "file_path = os.path.join(CURR_DIR, '../../../../DATA/dp0_train_random.csv')\n",
    "full_data = pd.read_csv(file_path, usecols=data_columns)\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spliting into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(full_data)//2\n",
    "\n",
    "train_sample = full_data.sample(n=size,ignore_index=True)\n",
    "test_sample = full_data.drop(train_sample.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##  RAIL \n",
    "\n",
    "Rail has a lot of classes and it uses Object Oriented Programming - OOP, therefore things can get complicated very fast, but for now we are going to focus on understanging a little bit of the three bases ones: **RailStage, DataStore and DataHandler**\n",
    "\n",
    "**Image:** This diagram represents some classes and its hierarchy.\n",
    "\n",
    "![title](RAILclasses.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DataStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data store class is the class that is going to store all the data that is being processed associated with a key value. For example for a file containing the sample that we are going to use to test an algorithm named 'test_sampe.hdf5' we add this to the data store naming the key 'test_sample' and a what class (DaataHandler) is it going to use to read it, in this case TableHandler -> HandlerHdf5. \n",
    "<br>\n",
    "\n",
    "Another important thing to know is that the DataStore class acts as a [singleton class](https://refactoring.guru/design-patterns/singleton) wich basically is a class that has only one instance in the aplication. That is important due to the fact that rail keeps all the data and handlers as it runs so that the previous stage can access and read it. Based on that when if we try to create another instace, what its going to do is serve as a DataStore factory, but not the DataStore class itself.  \n",
    "<br>\n",
    "We can see access the data storage trough the attribute data_store. By default it does not allow to overwrite the data tha its being stored so if we want to change the value of a key we have to manually set the property allow_overwrite to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_store():\n",
    "    print(DS)\n",
    "    print(sys.getsizeof(DS), 'bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us undestand better we are constantly going to monitor how DataStore stores the data and how the memory goes with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_store():\n",
    "    print(DS)\n",
    "    print(sys.getsizeof(DS), 'bytes')\n",
    "\n",
    "print_data_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually add data to the data store with the add_data or pass a file and store it in the DS with the read_file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS.add_data(key=\"input\", data=train_sample, handle_class=PqHandle)\n",
    "## DS.read_file(key=\"name\", path=file_path, handle_class=Handler)\n",
    "print_data_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we can access the data, what it is going to do is use the handler that we passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS.read(\"input\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory x Files\n",
    "\n",
    "As we can see, as soon that we added the data to the DS the memory increased in 200 bytes. In Rail we can store data as a tableLike object as pandas dataframe, orderDic, etc. but we can also work with the flow in memory. For that there are a bunch of steps/configs that are going be different from bringing or not the data to memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DataHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all the stages herd from RailStages, [Delegate Pattern](https://en.wikipedia.org/wiki/Delegation_pattern), that can be seen in figure of the classe maped above, and RailStages can be seen as a \n",
    "[CeciStage](https://github.com/LSSTDESC/ceci/blob/d1d5686aefab18bc53e3d4d8a05af42d19e28a91/ceci/stage.py#L24]), when we declare a stage we use the method _make_stage(**args)_, what id does is to return the object itself as a stage configured with the given parameters. \n",
    "\n",
    "To undestand how the returned object works we can use an explanation present in the c# language for [delegate](https://docs.microsoft.com/pt-br/dotnet/csharp/programming-guide/delegates/using-delegates). Basically we can think of delegates as a method that points to an abstract class and a method of that class that is going to execute. Therefore a class that behaves as a method and can be executed. In python this method can be declared as `__call__` and the retuned class can be executed as class(), then this is going to execute the defined methos class. For RailStages it is going to run the algorithm.\n",
    "\n",
    "**Image:** basic flux of inputs and outputs. \n",
    "\n",
    "![title](SimpleRailBPZflow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PRIOR - Preparing Data\n",
    "Here we are going to start preparing our data, first we set the ColumnMapper stage that is going to remmap all the data columns for bpz algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##help(ColumnMapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_map = {\n",
    "\"coadd_objects_id\": \"id\",\n",
    "\"ra\": \"coord_ra\",\n",
    "\"dec\": \"coord_dec\",\n",
    "\"mag_g\": \"mag_g_lsst\",\n",
    "\"magerr_g\": \"mag_err_g_lsst\",\n",
    "\"mag_r\": \"mag_r_lsst\",\n",
    "\"magerr_r\": \"mag_err_r_lsst\",\n",
    "\"mag_i\": \"mag_i_lsst\",\n",
    "\"magerr_i\": \"mag_err_i_lsst\",\n",
    "\"mag_u\": \"mag_u_lsst\",\n",
    "\"magerr_u\": \"mag_err_u_lsst\",\n",
    "\"mag_y\": \"mag_y_lsst\",\n",
    "\"magerr_y\": \"mag_err_y_lsst\",\n",
    "\"mag_z\": \"mag_z_lsst\",\n",
    "\"magerr_z\": \"mag_err_z_lsst\",\n",
    "\"z_true\": \"redshift\"\n",
    "}\n",
    "\n",
    "col_remapper = ColumnMapper.make_stage(name='col_remapper', columns=columns_map)\n",
    "print(f\"Returned class: {type(col_remapper)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_data_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the configurations of the returned class with `returned_obj.config.to_dict()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_remapper.config.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically we can call execute it these ways using the notebook.\n",
    "1. When the data is added manually to the DS with `col_remapper.run()`\n",
    "2. Passing the data trough parameter and invoking the method as `col_remapper(dataAsTableLike)`\n",
    "\n",
    "in this case we are going to call the method run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remembering what data we are storing\n",
    "DS.read(\"input\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_remapper.run()\n",
    "print(f\"\\nRodando em paralelo -> {col_remapper.is_parallel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it is storing the outputs in the DS before the stage. Lets check the outupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_remapper.get_data(\"output\").head() \n",
    "## or trough DS as \n",
    "##DS.read(\"output_col_remapper\")\n",
    "##DS[\"output_col_remapper\"].data\n",
    "\n",
    "#tables_io.convertObj(DS.read(\"output_estimate_bpz\").build_tables()['ancil'], tables_io.types.PD_DATAFRAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_data_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "**OBSERVATION**\n",
    "\n",
    "Passing the input key in make_stage does not work.\n",
    "`ColumnMapper.make_stage(name='col_remapper_train_2', columns=columns_map, input='test')`\n",
    "\n",
    "what it does is search for the key in DS with the 'input' name when we call the stage.run(), so if we want to call it that way it would be necessary to keep updating the value of intput key. To correct that it would be necessary to call the method that is responsable for setting the key name for when the stage runs it search for the right key.\n",
    "<br>\n",
    "Setting the key passed in the make_datege(input=\"key\"):\n",
    "<br>`self.set_data(self.config.input, data)`\n",
    "\n",
    "How the algorithm search in the .run() method:\n",
    "<br>`data = self.get_data('input', allow_missing=True)` <br>\n",
    "\n",
    "\n",
    "**OBS:** when we call the function as _stage(data)_ its not necessary to set the input key, it already does as the sugestion above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS.add_data(key=\"input\", data=col_remapper.get_data(\"output\"), handle_class=PqHandle)\n",
    "print(DS)\n",
    "DS.read(\"input\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PRIOR - Inform BPZ\n",
    "\n",
    "For the The algorithms, basically they all expect an input as TableHandler.<br>\n",
    "`inputs = [('input', <class 'rail.core.data.TableHandle'>)]`<br>\n",
    "as the output of remmapColumns is already a TableHandler we dont need to specify, but if the data is already in the correct form, it may be helpful to use the TableConverter class. \n",
    "\n",
    "Eg:\n",
    "\n",
    "     table_conv_train = TableConverter.make_stage(name='table_conv_train', output_format='numpyDict')\n",
    "     table_conv_train.run()\n",
    "\n",
    "\n",
    "and the output is a ModelHandler<br>\n",
    "`outputs = [('model', <class 'rail.core.data.ModelHandle'>)]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to set the data to input in DS so that when inform runs it gets the remmaped data in with the key 'input' in the DataStore  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "DS.read(\"input\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the InformBpz to define the priors to bpz algorithm, here we can configure all the parameters such as  zmin, zmax, etc.\n",
    "\n",
    "According to the [documentation BPZ_lite](https://github.com/LSSTDESC/rail_bpz/blob/main/src/rail/estimation/algos/bpz_lite.py) Inform:\n",
    "\n",
    "> Inform stage for BPZ_lite, this stage *assumes* that you have a set of\n",
    "    SED templates and that the training data has already been assigned a\n",
    "    'best fit broad type' (that is, something like ellliptical, spiral,\n",
    "    irregular, or starburst, similar to how the six SEDs in the CWW/SB set\n",
    "    of Benitez (2000) are assigned 3 broad types).  This informer will then\n",
    "    fit parameters for the evolving type fraction as a function of apparent\n",
    "    magnitude in a reference band, P(T|m), as well as the redshift prior\n",
    "    of finding a galaxy of the broad type at a particular redshift, p(z|m, T)\n",
    "    where z is redshift, m is apparent magnitude in the reference band, and T\n",
    "    is the 'broad type'.  We will use the same forms for these functions as\n",
    "    parameterized in Benitez (2000).  For p(T|m) we have\n",
    "    p(T|m) = exp(-kt(m-m0))\n",
    "    where m0 is a constant and we fit for values of kt\n",
    "    For p(z|T,m) we have\n",
    "    P(z|T,m) = f_x*z0_x^a *exp(-(z/zm_x)^a)\n",
    "    where zm_x = z0_x*(km_x-m0)\n",
    "    where f_x is the type fraction from p(T|m), and we fit for values of\n",
    "    z0, km, and a for each type.  These parameters are then fed to the BPZ\n",
    "    prior for use in the estimation stage.\n",
    "    \n",
    "   I dont rlly undestand everything here scientifically but we can move on for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(Inform_BPZ_lite)\n",
    "bpz_columns_file = os.path.join(CURR_DIR, '../configs/bpz.columns')\n",
    "bpz_columns_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "inform_bpz = Inform_BPZ_lite.make_stage(\n",
    "    name='inform_bpzlite', \n",
    "    #input=\"test_nome\",\n",
    "    model='trained_BPZ_output.pkl', \n",
    "    hdf5_groupname='', \n",
    "    columns_file=bpz_columns_file,\n",
    "    prior_band=\"mag_i_lsst\"\n",
    ")\n",
    "#inform_bpz.connect_input(col_remapper_train) # If we do not want to keep updating the input key in DS, we can connect it like this\n",
    "inform_bpz.config.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Inform to compute the best fit prior parameters. What is fo and kt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "inform_bpz.run()\n",
    "## or inform_bpz.inform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_data_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "inform_bpz.config.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is interesting to see the aliases key and as we run it sets the model and the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS['model_inform_bpzlite'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## BPZ Estimate -  Preparing Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For posteriors\n",
    "\n",
    "     inputs = [('model', <class 'rail.core.data.ModelHandle'>)]\n",
    "     outputs = [('output', <class 'rail.core.data.QPHandle'>)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding data to DS key input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "DS.add_data(key=\"input\", data=test_sample, handle_class=ModelHandle)\n",
    "print_data_store()\n",
    "DS.read(\"input\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_remapper.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Estimate stage requires a different type of input data, either a dictionary of all input data or a ModelHandle providing access to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS.add_data(key=\"input\", data=col_remapper.get_data(\"output\"), handle_class=PqHandle)\n",
    "print_data_store()\n",
    "DS.read(\"input\").head()\n",
    "\n",
    "table_conv = TableConverter.make_stage(name='table_conv', output_format='numpyDict');\n",
    "## or set the input as \n",
    "# table_conv.connect_input(col_remapper_test)\n",
    "table_conv.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding the result to input in DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS.add_data(key=\"input\", data=table_conv.get_data(\"output\"), handle_class=PqHandle)\n",
    "print_data_store()\n",
    "DS.read(\"input\")['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## BPZ Estimate - run\n",
    "\n",
    "Now that we have our test sample in the input key in DS we define and run the algorithm. Here we can change the configs such as zmin, zmax, dx, bins, etc, via parameter fow each run that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimate_bpz = BPZ_lite.make_stage(\n",
    "    name='estimate_bpz', \n",
    "    hdf5_groupname='', \n",
    "    columns_file=bpz_columns_file, \n",
    "    #input=\"inprogress_output_table_conv_train.hdf5\", \n",
    "    model=inform_bpz.get_handle('model'),\n",
    "    zmax=1.5\n",
    ")\n",
    "#estimate_bpz.connect_input(table_conv)\n",
    "estimate_bpz.config.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_bpz.run() ## -> input -> DS\n",
    "#estimate(data) ## 'input' -> dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_data_store()\n",
    "estimate_bpz.get_data(\"output\").build_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "DS.read(\"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ztrue = DS.read(\"input\")\n",
    "DS.add_data(key=\"truth\", data=ztrue, handle_class=TableHandle)\n",
    "print_data_store()\n",
    "DS.read(\"truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = DS.read(\"output_estimate_bpz\")\n",
    "DS.add_data(key=\"input\", data=ensemble, handle_class=QPHandle)\n",
    "print_data_store(), type(ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator.make_stage(name=f'bpz_eval', truth=ztrue)\n",
    "# evaluator.connect_input(estimate_bpz)\n",
    "evaluator.run()\n",
    "#evaluator.evaluate(ensemble, ztrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_data_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_io.convertObj(DS.read(\"output_bpz_eval\"), tables_io.types.PD_DATAFRAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "____\n",
    "\n",
    "## Plot pz x zspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tables = tables_io.convertObj(DS.read(\"output_estimate_bpz\").build_tables()['ancil'], tables_io.types.PD_DATAFRAME)\n",
    "zmode = results_tables['zmode']\n",
    "len(zmode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(train_sample['z_true'],zmode,s=1,c='k',label='simple bpz mode')\n",
    "plt.plot([0,3],[0,3],'r--');\n",
    "plt.xlabel(\"true redshift\")\n",
    "plt.ylabel(\"bpz photo-z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION\n",
    "\n",
    "As we can see some PITs were not calculated and the plot has all the dots highly scattered, that makes some sense with why it cut down the PITs values, since there can be some error in sample configuration as a switched column or another input error that could not be detected. However, for this notebook, is important to understand how we can run rail and how its stages work together using the notebook as the place to run. It is important to say that we can use ceci to run all the same stages that were configured here, but this is going to be explored in another notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
